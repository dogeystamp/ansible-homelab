---
# See meta/argument_specs.yml for information on how to use this role.

- name: Create VM home directory
  ansible.builtin.file:
    name: "{{ libvirt_vm_directory }}"
    state: directory
    owner: root
    group: root
    mode: "u=rwx,g=x,o=x"

- name: Create VM user
  ansible.builtin.user:
    name: "{{ libvirt_vm_name }}"
    state: present
    create_home: true
    home: "{{ libvirt_vm_directory }}/{{ libvirt_vm_name }}"
    shell: "/usr/sbin/nologin"
  register: vm_user

- name: Create VM system directory
  ansible.builtin.file:
    path: "{{ vm_user.home }}/machine"
    state: directory
    owner: "{{ vm_user.uid }}"
    group: root
    mode: "u=rwx,g=,o="
  register: vm_dir

- name: Create VM data directory
  ansible.builtin.file:
    path: "{{ vm_user.home }}/vmdata"
    state: directory
    owner: "{{ vm_user.uid }}"
    group: root
    mode: "u=rwx,g=,o="
  register: vm_data_dir

# WireGuard configuration is created and never deleted, because
# otherwise we can't read the public key out of the VM easily. This
# should not create any extra security risks, because the VM user has
# access to the VM image anyways, so they already have access to the key.
- name: Create WireGuard configuration directory
  ansible.builtin.file:
    path: "{{ vm_dir.path }}/vpn"
    state: directory
    owner: "{{ vm_user.uid }}"
    group: root
    mode: "u=rwx,g=,o="
  register: vpn_dir

- name: Create VM WireGuard setup
  ansible.builtin.import_role:
    name: wireguard
  vars:
    wireguard_host: "{{ libvirt_vm_name }}"
    wireguard_interface: "wg1"
    wireguard_addresses: "{{ libvirt_vpn_addresses }}"
    wireguard_description: "VPN ONE â€” {{ libvirt_vm_name }}"
    wireguard_file_owner: "{{ vm_user.name }}"
    wireguard_peers:
      # this is the hypervisor pubkey
      - public_key: "{{ hostvars.localhost['wireguard_public_keys_' + libvirt_vpn_interface][inventory_hostname] }}"
        allowed_ips:
          - "0.0.0.0/0"
        # QEMU host IP (in passt rootless networking)
        # not really sure if this is always the case...
        endpoint: "192.168.0.1:51820"
        persistent_keepalive: 25
    wireguard_path: "{{ vpn_dir.path }}"
    wireguard_immediate: false

- name: Get VM image path
  ansible.builtin.set_fact:
    vm_image_path: "{{ vm_dir.path }}/{{ libvirt_clean_image | basename }}"

- name: Get ext image path
  ansible.builtin.set_fact:
    ext_image_path: "{{ vm_dir.path }}/{{ libvirt_clean_ext_image | basename }}"

- name: Check if VM image already exists
  ansible.builtin.stat:
    path: "{{ vm_image_path }}"
    get_checksum: false  # checksumming large files is inconvenient
    get_mime: false
  register: vm_image

- name: Check if ext image already exists
  ansible.builtin.stat:
    path: "{{ ext_image_path }}"
    get_checksum: false  # checksumming large files is inconvenient
    get_mime: false
  register: ext_image

- name: Copy clean ext disk image
  ansible.builtin.copy:
    src: "{{ libvirt_clean_ext_image }}"
    remote_src: true
    dest: "{{ ext_image_path }}"
    owner: "{{ vm_user.uid }}"
    group: root
    force: false
    mode: "u=rw,g=,o="
  when: "libvirt_enable_ext and not ext_image.stat.exists"

- name: List existing VMs
  community.libvirt.virt:
    uri: "qemu:///session"
    command: list_vms
  register: current_vms
  changed_when: false
  become_user: "{{ vm_user.name }}"
  become: true

- name: Create VM first-boot image
  when: "libvirt_vm_name not in current_vms.list_vms and not vm_image.stat.exists"
  block:
    - name: Copy clean VM disk image
      ansible.builtin.copy:
        src: "{{ libvirt_clean_image }}"
        remote_src: true
        dest: "{{ vm_image_path }}"
        owner: "{{ vm_user.uid }}"
        group: root
        force: false
        mode: "u=rw,g=,o="

    - name: Deploy first-boot provision script
      ansible.builtin.template:
        src: "provision.sh.j2"
        dest: "{{ vm_dir.path }}/provision.sh"
        owner: "{{ vm_user.uid }}"
        group: root
        mode: "u=rw,g=,o="
      register: vm_provision_script

    - name: Customize VM disk image
      ansible.builtin.command:
        argv:
          - "virt-customize"
          - "-a"
          - "{{ vm_image_path }}"
          - "--firstboot"
          - "{{ vm_provision_script.dest }}"
          - "--hostname"
          - "{{ libvirt_vm_name }}"
      become_user: "{{ vm_user.name }}"
      become: true
      changed_when: true

    - name: Deploy WireGuard configuration to VM
      ansible.builtin.command:
        argv:
          - "virt-customize"
          - "-a"
          - "{{ vm_image_path }}"
          - "--copy-in"
          - "{{ vpn_dir.path }}:/root/"
      become_user: "{{ vm_user.name }}"
      become: true
      changed_when: true

- name: Register VM in libvirt
  when: "libvirt_vm_name not in current_vms.list_vms"
  block:
    - name: Define VM
      community.libvirt.virt:
        uri: "qemu:///session"
        autostart: "{{ libvirt_autostart }}"
        command: "define"
        xml: "{{ lookup('template', 'vm.xml.j2') }}"
      become_user: "{{ vm_user.name }}"
      become: true

    - name: Start VM
      when: "libvirt_vm_name not in current_vms.list_vms"
      # WARNING:
      # because sudo can't appropriately set a user session for libvirt, we sudo run0.
      # notably, XDG_RUNTIME_DIR is needed for passt + SELinux, otherwise you get this error:
      # "unexpected exit status 1: Failed to bind UNIX domain socket: Permission denied"
      # as passt tries to bind in .cache, but it only has permissions to bind in /run/.
      #
      # there is also this error:
      # "libvirt: error : cannot limit core file size of process 3666 to 18446744073709551615: Operation not permitted"
      # (https://github.com/secureblue/secureblue/issues/1054)
      # but I don't know how that can be fixed. the solution is just use run0 (or log in) instead of sudo.
      #
      # note also that using sudo -u then running `virsh list` will result in VMs being displayed
      # as "shut off", even though they are running.
      # see: https://bbs.archlinux.org/viewtopic.php?pid=2183213#p2183213
      ansible.builtin.command:
        argv:
          - run0
          - "-u"
          - "{{ vm_user.name }}"
          - "/usr/bin/virsh"
          - start
          - "{{ libvirt_vm_name }}"
      become: true
      register: vm_start_cmd
      failed_when: "vm_start_cmd.rc != 0 and 'Domain is already active' not in vm_start_cmd.stderr"
      changed_when: true

- name: Create systemd user service directory
  ansible.builtin.file:
    path: "~/.config/systemd"
    state: directory
    owner: "{{ vm_user.name }}"
    group: "{{ vm_user.name }}"
    mode: u=rwx,g=,o=
  become_user: "{{ vm_user.name }}"
  become: true

- name: Create systemd user service directory
  ansible.builtin.file:
    path: "~/.config/systemd/user"
    state: directory
    owner: "{{ vm_user.name }}"
    group: "{{ vm_user.name }}"
    mode: u=rwx,g=,o=
  become_user: "{{ vm_user.name }}"
  become: true

- name: Create systemd user service to wake up libvirt
  ansible.builtin.template:
    src: vm-wakeup.service
    dest: "~/.config/systemd/user/vm-wakeup.service"
    force: false
    owner: "{{ vm_user.name }}"
    group: "{{ vm_user.name }}"
    mode: u=rw,g=,o=
  become_user: "{{ vm_user.name }}"
  become: true
  register: systemd_service

- name: Enable VM wakeup service
  ansible.builtin.command:
    argv:
      - machinectl
      - shell
      - "{{ vm_user.name }}@"
      - "/usr/bin/systemctl"
      - "--user"
      - enable
      - "--now"
      - "vm-wakeup.service"
  when: systemd_service.changed and libvirt_autostart
  changed_when: true

- name: Enable linger for user
  ansible.builtin.command:
    argv:
      - loginctl
      - enable-linger
      - "{{ vm_user.name }}"
  when: systemd_service.changed and libvirt_autostart
  changed_when: true
